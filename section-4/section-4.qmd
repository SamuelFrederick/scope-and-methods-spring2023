---
title: "Section 4: Statistical Relationships"
author: "Sam Frederick"
institute: "Columbia University"
format: 
  revealjs:
    css: [footer.css]
    theme: [default, custom.scss]
    chalkboard: true
    progress: true
    include-after-body: footer.html 
    footer: "Statistical Relationships"
knitr: 
  opts_chunk: 
      echo: true
---

## Today's Section

- Correlation Coefficients

- Law of Large Numbers

- The Central Limit Theorem

- Hypothesis Testing

## Correlation Coefficients

:::{.fragment}
- Statistically summarize relationships between numeric variables
:::
:::{.fragment}
- Range from -1 to 1
:::
:::{.fragment}
- Values closer to -1 or 1 indicate stronger relationships
:::

## Correlation Coefficients

$Cor(X, Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)*Var(Y)}} = \frac{\frac{1}{n-1}\sum_{i=1}^n(X-\bar{X})(Y-\bar{Y})}{\sqrt{\frac{1}{n-1}\sum_{i=1}^n(X-\bar{X})^2 *\frac{1}{n-1} \sum_{i=1}^n(Y-\bar{Y})^2}}$

Equivalently:

$Cor(X,Y) = \frac{\sum_{i=1}^n (X-\bar{X})*(Y-\bar{Y})}{ \sqrt{\sum_{i=1}^n(X-\bar{X})^2 *\sum_{i=1}^n(Y-\bar{Y})^2}}$

## Correlation Coefficients in R

```{r}
x <- c(1, 3, 5, 7, 9)
y <- c(1, 4, 7, 2 , 10)
```

:::{.fragment}
- By hand: 

```{r}
numerator <- sum((x-mean(x))*(y - mean(y)))
denominator <- sqrt(sum((x-mean(x))^2)*sum((y - mean(y))^2))
round(numerator/denominator, 3)
```
:::
:::{.fragment}
- Using R functions:

```{r}
round(cov(x, y)/sqrt(var(x)*var(y)), 3)
```
:::
:::{.fragment}
- Using one R function:

```{r}
round(cor(x, y), 3)
```
:::

## Custom Functions in R

```{r, eval = F}
name <- function(arguments){
  tasks
  return(output)
}
name(arguments)
```


## Custom Functions in R

```{r}
square <- function(x) {
  return(x^2)
}
square(2)
square(4)
```

## Custom Functions in R

```{r}
correlation <- function(x, y) {
  numerator <- sum((x-mean(x))*(y - mean(y)))
  denominator <- sqrt(sum((x-mean(x))^2)*sum((y - mean(y))^2))
  return(round(numerator/denominator, 3))
}
correlation(x,y)
```



## Law of Large Numbers

As $n \to \infty$, sample mean approaches true population mean:

```{r, echo = F}
library(tidyverse)
set.seed(123)
pop <- rnorm(1000000, 375, 1000)
samp_sizes <- seq(1, 10000, 5)
samp_means <- sapply(samp_sizes, 
       function(x) mean(sample(pop, size = x, replace = T)))
tibble(samp_mean = samp_means, 
       samp_size = samp_sizes) %>%
  ggplot(aes(samp_size, samp_mean)) + geom_line() +
  geom_hline(yintercept = mean(pop), col = "red", lty = "dashed") +
  labs(x = "Sample Size", y = "Sample Mean", title = "Law of Large Numbers") 
```

## Central Limit Theorem

- As $n \to \infty$, $\sqrt{n}(\bar{X}_n-\mu) \to N(0, \sigma^2)$

```{r, echo = F}
set.seed(12345)
pop <- sample(1:30, 1000000, replace = T)
clt <- lapply(c(1:30, 50,75, 100,250, 500, 
                1000,2500,  5000, 7500, 10000, 100000), 
       function(x) replicate(200, mean(sample(pop, size = x, replace = T))))

```

## Central Limit Theorem

- What's more,  As $n \to \infty$, $\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \to N(0, 1)$

```{r, echo = F, cache = T}
a <- tibble(sample = unlist(clt),
       sampsize = rep(c(1:30, 50,75, 100,250, 500, 1000,2500,  5000, 7500, 10000, 100000), lengths(clt))) %>%
  group_by(sampsize) %>%
  mutate(sd_samp = sd(pop)/sqrt(sampsize)) %>%
  ungroup() %>%
  mutate(sample =(sample - mean(pop))/(sd_samp), 
         sampsize = factor(sampsize)) %>% 
  ggplot(aes(sample)) + 
    geom_histogram(aes(y = ..density..)) +
    stat_function(fun = dnorm, color = "red") + 
    labs(x = "Sample Means", y = "Density", title = "Sample Size: {closest_state}")+
  # facet_wrap(~sampsize)
    gganimate::transition_states(sampsize,transition_length = 0)
gganimate::animate(a, duration = 10)
```


## Hypothesis Testing

- Null Hypothesis:
  - Usually that true population mean equal to some value ($\mu = x$)
  - e.g., the true approval rate of Joe Biden is 50%
  - e.g., the difference between the conservatism of Democrats and Republicans is 0
  
- Alternative Hypothesis:
  - Two-sided: $\mu \neq x$
  - One-sided: $\mu > x$ or $\mu < x$

## Hypothesis Testing

- Calculate the Z-Score based on null hypothesis:
  - $Z = \frac{\bar{X} - \mu_{0}}{\sigma/\sqrt{n}}$
  - Two-Sample/Difference-in-Means Test:
    - $Z = \frac{(\bar{X} - \bar{Y}) - (\mu_{0x}-\mu_{0y})}{\sqrt{\sigma_x^2/n_x + \sigma_y^2/n_y}}$
    
## Hypothesis Testing

- Under the Central Limit Theorem, the Z-Score should be 
  - distributed approximately standard normal 
    - **if** we repeated the sampling process many times with a large enough sample
    - and **if** the null hypothesis is true
    
## Hypothesis Testing

- Standard Normal Distribution has known properties
  - Calculate probability of observing Z-Score at least as large as observed Z-Score
    - **if** the null hypothesis is true
  - This is the **p-value**
  
## Hypothesis Testing: Example

```{r}
nominate <- read_csv("~/Downloads/HSall_members.csv") %>%
  filter(congress==118&party_code%in%c(100, 200)&
           chamber!="President") 
dem <- nominate %>% filter(party_code==100) %>% drop_na(nominate_dim1)
rep <- nominate%>% filter(party_code==200) %>% drop_na(nominate_dim1)
diff_in_means <- mean(rep$nominate_dim1) - mean(dem$nominate_dim1)
denominator <- sqrt((var(rep$nominate_dim1, na.rm = T)/nrow(rep)) +
                      (var(dem$nominate_dim1)/nrow(dem)))
z_score <- diff_in_means/denominator
round(z_score, 3)
```

## Hypothesis Testing: Example

```{r}
ggplot()+
  stat_function(fun = dnorm) + 
  xlim(c(-3, 3)) + 
  labs(x = "X", y = "Density", 
       title = "Standard Normal Distribution")
```

## Hypothesis Testing: Example

```{r}
ggplot()+
  stat_function(fun = dnorm) + 
  xlim(c(-100,100)) + 
  labs(x = "X", y = "Density", 
       title = "Standard Normal Distribution") + 
  geom_vline(xintercept = z_score, color = "red", lty = "dashed")
```

## Hypothesis Testing: Example

- One-Sided Hypothesis Test: $\mu_{0R} - \mu_{0D} >0$

```{r}
1-pnorm(z_score, mean = 0, sd = 1)
```

- Two-Sided Hypothesis Test: $\mu_{0R}- \mu_{0D} \neq 0$

```{r}
2*(1-pnorm(z_score, mean = 0, sd = 1))
```

## Hypothesis Testing: T-Distribution

- Often use the t distribution instead of normal distribution 
  - especially with small sample sizes

- t-distribution places more probability in the tails

- In large samples, the t-distribution is equivalent to the normal distribution


## T Distribution

```{r, echo = F}
ggplot() + 
  stat_function(fun = dnorm) + 
  stat_function(fun = dt, args = list(df = 4), color = "red") + 
  xlim(c(-3,3)) + 
  annotate(geom = "text", x = -2.5, y = 0.1, label = "t-distribution", 
           color= "red") + 
  annotate(geom= "text", x = 1.5, y = 0.3, label = "Standard Normal") +
  labs(x = "X", y = "Density")
```


## Hypothesis Testing

- In R, can use the `t.test()` function

```{r}
t.test(rep$nominate_dim1, dem$nominate_dim1)
```

## Confidence Intervals

- We often want some idea of uncertainty in our estimates

- Use Central Limit Theorem to construct "confidence intervals" around our estimates

- Lower End: sample estimate - $qnorm(0.975)*$Standard Error
- Upper End: sample estimate + $qnorm(0.975)*$Standard Error

## Confidence Intervals: Example

```{r}
dem_mean <- mean(dem$nominate_dim1)
dem_mean - qnorm(0.975)*sd(dem$nominate_dim1)/sqrt(nrow(dem))
dem_mean
dem_mean + qnorm(0.975)*sd(dem$nominate_dim1)/sqrt(nrow(dem))
```

## Confidence Intervals: Example

```{r, echo = F}
lb <- dem_mean - qnorm(0.975)*sd(dem$nominate_dim1)/sqrt(nrow(dem))
ub <- dem_mean + qnorm(0.975)*sd(dem$nominate_dim1)/sqrt(nrow(dem))
tibble(lb = lb, mean = dem_mean, ub = ub, party = "Democrat") %>%
  ggplot(aes(mean, party)) + 
  geom_point() + 
  geom_errorbar(aes(y = party, xmin = lb, xmax = ub, width = 0.1))
```

## Confidence Intervals: Example

```{r, echo = F}
lb <- dem_mean - qnorm(0.975)*sd(dem$nominate_dim1)/sqrt(nrow(dem))
ub <- dem_mean + qnorm(0.975)*sd(dem$nominate_dim1)/sqrt(nrow(dem))
tibble(lb = lb, mean = dem_mean, ub = ub, party = "Democrat") %>%
  ggplot(aes(mean, party)) + 
  geom_point() + 
  geom_errorbar(aes(y = party, xmin = lb, xmax = ub, width = 0.1)) + 
  xlim(c(-0.4, 0.1)) + geom_vline(xintercept = 0, lty = "dashed")
```


## Confidence Intervals

- Most common: 95% "Confidence Intervals"
  - Does **NOT** mean we are 95% confident that the true population value is in the interval

:::{.fragment}
  - Real meaning: if we repeat the sampling process 100 times, 95% of the 95% confidence intervals will contain the true population value (on average)
:::

## Confidence Intervals

```{r}
set.seed(123)
n <- 100000
pop <- rnorm(n, 15, 10)
samp <- sample(pop, size = 100)
samp_mean <- mean(samp)
lb <- samp_mean - qnorm(0.975, mean = 0, sd = 1)*sd(samp)/sqrt(100)
ub <- samp_mean +qnorm(0.975, mean= 0, sd = 1)*sd(samp)/sqrt(100)
```

```{r, echo = F}
tibble(mean= samp_mean, lb = lb, ub = ub, samp = "Sample") %>%
  ggplot(aes(mean, samp)) +
  geom_point() +
  geom_errorbar(aes(xmin = lb, xmax = ub, y = samp), width = 0.1) + 
  geom_vline(xintercept = mean(pop), lty = "dashed", color = "blue")
```

## Confidence Intervals

```{r, echo = F}
set.seed(1997)
n <- 100000
pop <- rnorm(n, 15, 10)
samp_fun<- function(x){
  samp <- sample(pop, size = 100)
  samp_mean <- mean(samp)
  lb <- samp_mean - qnorm(0.975)*sd(samp)/sqrt(100)
  ub <- samp_mean +qnorm(0.975)*sd(samp)/sqrt(100)
  return(tibble(mean =samp_mean, lb = lb, ub = ub, samp = x))
}
res <- data.table::rbindlist(lapply(1:100, samp_fun)) %>%
  as_tibble() %>% 
  mutate(true_in_int = map2_lgl(lb, ub, ~between(mean(pop), .x, .y)))


```

```{r, echo = F}
res %>%
  ggplot(aes(mean, reorder(samp, mean), color = true_in_int)) +
  geom_point() +
  geom_errorbar(aes(xmin = lb, xmax = ub, y = reorder(samp, mean)), width = 0.1) + 
  geom_vline(xintercept = mean(pop), lty = "dashed", color = "blue") +
  labs(x = "Sample Mean", y= "Sample") +
  theme(axis.text.y = element_blank())
```

## Recap

- Central Limit Theorem (and Law of Large Numbers) central to many scientific tasks

- Used for calculating p-values, hypothesis testing, and constructing confidence intervals

- p-value: probability of observing a Z-score/t-statistic at least as large as the one actually observed **if** the null hypothesis is true

## Recap 

- Confidence Intervals: 
  - lower bound: sample estimate - $qnorm(0.975)*$Standard Error
  - upper bound: sample estimate + $qnorm(0.975)*$Standard Error

- We are **NOT** 95% confident that the true population value is in the interval
  - Only that, if we repeated the sampling process many times, roughly 95% of the intervals would contain the true population mean